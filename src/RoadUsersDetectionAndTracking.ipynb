{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from cv2 import VideoWriter, VideoWriter_fourcc\n",
    "from ipynb.fs.full.RoadUsersTrackingUtil import *\n",
    "from ipynb.fs.full.TrafficLightDetection import *\n",
    "from ipynb.fs.full.VideoProcessingUtil import *\n",
    "\n",
    "def applyMaskToFrame(mog2BackgroundSubtractor, regionOfInterest):\n",
    "    \n",
    "    # Applying the mask on the region of interest using the Background Subtractor MOG2\n",
    "    frameMask = mog2BackgroundSubtractor.apply(regionOfInterest)\n",
    "    \n",
    "    # Removing the non-white (gray) pixels from the shadows of the foreground objects present in the frame\n",
    "    _, frameMask = cv2.threshold(frameMask, 254, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Obtaining the co-ordinates of the moving objects (road users) present in the foreground of the frame\n",
    "    contours, hierarchy = cv2.findContours(frameMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    return frameMask,contours\n",
    "\n",
    "def detectAndTrackRoadUsers(cameraStreamVideoFile, roiY1, roiYn, roiX1, roiXn):\n",
    "    \n",
    "    # Traffic lights detection constants\n",
    "    TRAFFIC_LIGHTS_DNN_MODEL='../models/yolov3_training_final.weights'\n",
    "    TRAFFIC_LIGHTS_DNN_CFG='../models/yolov3_testing.cfg'\n",
    "    TRAFFIC_LIGHTS_DNN_CLASS_LABELS_FILE='../models/classes.txt'\n",
    "    \n",
    "    # Traffic enforcement camera constants\n",
    "    PROCESSED_OUTPUT_VIDEOS_FOLDER='../processed_videos/'\n",
    "    PROCESSED_OUTPUT_VIDEO_FILENAME2=os.path.join(PROCESSED_OUTPUT_VIDEOS_FOLDER,'2_Traffic_Lights_Detection.mp4')\n",
    "    PROCESSED_OUTPUT_VIDEO_FILENAME3=os.path.join(PROCESSED_OUTPUT_VIDEOS_FOLDER,'3_Determining_Red_Light_Violations.mp4')\n",
    "    OUTPUT_VIDEO_WIDTH=1920\n",
    "    OUTPUT_VIDEO_HEIGHT=1080\n",
    "    OUTPUT_VIDEO_FPS=30.26\n",
    "    OUTPUT_VIDEO_FOURCC=VideoWriter_fourcc(*'MP42')\n",
    "    \n",
    "    # Preparing the video writer for generating the traffic lights detection video\n",
    "    trafficLightsDetectionVideo = VideoWriter(PROCESSED_OUTPUT_VIDEO_FILENAME2, OUTPUT_VIDEO_FOURCC, float(OUTPUT_VIDEO_FPS), (OUTPUT_VIDEO_WIDTH, OUTPUT_VIDEO_HEIGHT))\n",
    "    \n",
    "    # Preparing the video writer for generating the red light violations video\n",
    "    determiningRedLightViolationsVideo = VideoWriter(PROCESSED_OUTPUT_VIDEO_FILENAME3, OUTPUT_VIDEO_FOURCC, float(OUTPUT_VIDEO_FPS), (OUTPUT_VIDEO_WIDTH, OUTPUT_VIDEO_HEIGHT))\n",
    "    \n",
    "    # Loading the custom-made model (.weights) file and the configuration (.cfg) file\n",
    "    deepNeuralNetwork=cv2.dnn.readNet(TRAFFIC_LIGHTS_DNN_MODEL,TRAFFIC_LIGHTS_DNN_CFG)\n",
    "    \n",
    "    # Creating a list of traffic light class labels\n",
    "    trafficLights = []\n",
    "\n",
    "    # Reading the traffic light class labels from the class labels file and storing it in the list for further processing\n",
    "    with open(TRAFFIC_LIGHTS_DNN_CLASS_LABELS_FILE, \"r\") as classLabels:\n",
    "        trafficLights = classLabels.read().splitlines()\n",
    "    \n",
    "    # Assigning the colour code randomly for the traffic light's bounding boxes\n",
    "    colours = np.random.uniform(0, 255, size=(100, 3))\n",
    "    \n",
    "    # Using the Gaussian mixture-based background and foreground segmentation algorithm\n",
    "    # to detect moving objects (road users) in the foreground against a stationary background (road)\n",
    "    mog2BackgroundSubtractor = cv2.createBackgroundSubtractorMOG2(history=100, varThreshold=40)\n",
    "    \n",
    "    # Creating a RoadUsersTrackingUtil class object for road user tracking\n",
    "    roadUsersTrackingUtil = RoadUsersTrackingUtil()\n",
    "    \n",
    "    # Read the video stream generated by module 1\n",
    "    cameraStreamVideo = VideoProcessingUtil(cameraStreamVideoFile).start()\n",
    "    time.sleep(1.0)\n",
    "    \n",
    "    print('###### Module 2 - Traffic light detection and aspect tracking begins ######')\n",
    "    print('RoadUsersDetectionAndTracking ---- Generating the traffic lights detection video . . .')\n",
    "    \n",
    "    # Reading the video stream frame-by-frame\n",
    "    while (cameraStreamVideo.next()):\n",
    "        frame = cameraStreamVideo.readQueue()\n",
    "        if np.shape(frame) == ():\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                height, width, _ = frame.shape\n",
    "            except AttributeError:\n",
    "                print('RoadUsersDetectionAndTracking ---- Attribute error in the current frame while processing.')\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            frame,redLightFlag=trafficLightDetectionAndTrack(frame,deepNeuralNetwork,trafficLights,colours)\n",
    "            copiedFrame=frame.copy()\n",
    "            trafficLightsDetectionVideo.write(copiedFrame)\n",
    "            \n",
    "            roadUserBoundingBoxes=[]\n",
    "            \n",
    "            # Setting the height and width for the traffic intersection as the region of interest\n",
    "            regionOfInterest = frame[roiY1: roiYn,roiX1: roiXn]\n",
    "            \n",
    "            # Applying the mask to the frame using Background Subtractor MOG2 and region of interest\n",
    "            frameMask,contours=applyMaskToFrame(mog2BackgroundSubtractor, regionOfInterest)\n",
    "            \n",
    "            for contour in contours:\n",
    "                \n",
    "                # Calculating the total area of the objects in the foreground so as to prevent the\n",
    "                # detection and tracking of smaller components present in these objects such as wheels and helmets\n",
    "                foregroundObjectArea = cv2.contourArea(contour)\n",
    "                \n",
    "                if foregroundObjectArea > 3000:\n",
    "                    # Bound only the bigger, detected foreground objects\n",
    "                    x, y, w, h = cv2.boundingRect(contour)\n",
    "                    \n",
    "                    # Append the values of the bounding box to a list for further processing\n",
    "                    roadUserBoundingBoxes.append([x, y, w, h])\n",
    "            \n",
    "            # Fetching the bounding box ids for all the bounding boxes\n",
    "            boundingBoxIds,crossedTrafficLightFlag = roadUsersTrackingUtil.getBoundingBoxIds(roadUserBoundingBoxes)\n",
    "            \n",
    "            # The primary logic to determine red light violation\n",
    "            for boundingBoxId in boundingBoxIds:\n",
    "                x, y, w, h, id = boundingBoxId\n",
    "                \n",
    "                # Check to see if the road user has crossed the stop line when the traffic light is red\n",
    "                if(redLightFlag and crossedTrafficLightFlag):\n",
    "                    print('RoadUsersDetectionAndTracking ---- Red light violation detected!')\n",
    "                    cv2.putText(regionOfInterest, 'VIOLATOR ' + str(id), (x, y - 15), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 2)\n",
    "                    cv2.rectangle(regionOfInterest, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "                    redLightFlag=False\n",
    "                    crossedTrafficLightFlag=False\n",
    "                else:\n",
    "                    cv2.putText(regionOfInterest, str(id), (x, y - 15), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 191, 0), 2)\n",
    "                    cv2.rectangle(regionOfInterest, (x, y), (x + w, y + h), (255, 191, 0), 2)\n",
    "            \n",
    "            # Write the frame to the video\n",
    "            determiningRedLightViolationsVideo.write(frame)\n",
    "    \n",
    "    print('RoadUsersDetectionAndTracking ---- The traffic lights detection video has been successfully generated.')\n",
    "    print('###### Module 2 - Traffic light detection and aspect tracking ends ######')\n",
    "    \n",
    "    print('###### Module 3 - Road user detection and tracking begins ######')\n",
    "    print('RoadUsersDetectionAndTracking ---- Generating the road user detection and tracking video . . .')\n",
    "    \n",
    "    print('RoadUsersDetectionAndTracking ---- The road user detection and tracking video has been successfully generated.')\n",
    "    print('###### Module 3 - Road user detection and tracking ends ######')\n",
    "    \n",
    "    # Release all the resources\n",
    "    determiningRedLightViolationsVideo.release()\n",
    "    trafficLightsDetectionVideo.release()\n",
    "    cameraStreamVideo.stop()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
