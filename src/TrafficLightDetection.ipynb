{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from cv2 import VideoWriter, VideoWriter_fourcc\n",
    "from ipynb.fs.full.VideoProcessingUtil import *\n",
    "\n",
    "def trafficLightDetectionAndTracking(modelFile,configFile,classLabelsFile,cameraStream,font,trafficLightsDetectionVideo):\n",
    "    \n",
    "    # Loading the custom-made model (.weights) file and the configuration (.cfg) file\n",
    "    deepNeuralNetwork=cv2.dnn.readNet(modelFile,configFile)\n",
    "    \n",
    "    # Creating a list of traffic light class labels\n",
    "    trafficLights = []\n",
    "    \n",
    "    # Reading the traffic light class labels from the class labels file and storing it in the list for further processing\n",
    "    with open(classLabelsFile, \"r\") as classLabels:\n",
    "        trafficLights = classLabels.read().splitlines()\n",
    "    \n",
    "    # Assigning the colour code randomly for the traffic light's bounding boxes\n",
    "    colours = np.random.uniform(0, 255, size=(100, 3))\n",
    "    \n",
    "    # Reading the video stream from the traffic enforcement/red light camera\n",
    "    videoCapture = cv2.VideoCapture(cameraStream)\n",
    "    \n",
    "    # Reading the video stream frame-by-frame\n",
    "    while (videoCapture.isOpened()):\n",
    "        ret, frame = videoCapture.read()\n",
    "        if np.shape(frame) == ():\n",
    "            break\n",
    "        else:\n",
    "            # Initializing the bounding boxes, confidence and the class ids for traffic light detection\n",
    "            boundingBoxes = []\n",
    "            classIds = []\n",
    "            confidence = []\n",
    "            \n",
    "            # Obtaining the frame's height and width\n",
    "            height, width, _ = frame.shape\n",
    "            \n",
    "            # Obtaining the four dimensional Binary Large OBject (BLOB) from the frame\n",
    "            blob = cv2.dnn.blobFromImage(frame, 1/255, (416, 416), (0,0,0), swapRB=True, crop=False)\n",
    "            \n",
    "            # Setting the BLOB as the input to the deep neural network (DNN)\n",
    "            deepNeuralNetwork.setInput(blob)\n",
    "            \n",
    "            # Retrieving the names of the unconnected output layers in the deep neural network and passing it to forward propagation\n",
    "            unconnectedOutputLayersNames = deepNeuralNetwork.getUnconnectedOutLayersNames()\n",
    "            layerOutputs = deepNeuralNetwork.forward(unconnectedOutputLayersNames)\n",
    "            \n",
    "            \n",
    "            for eachOutput in layerOutputs:\n",
    "                for eachDetection in eachOutput:\n",
    "                    \n",
    "                    # Obtaining the score, class id and the confidence for each detection\n",
    "                    scores = eachDetection[5:]\n",
    "                    classId = np.argmax(scores)\n",
    "                    conf = scores[classId]\n",
    "                    \n",
    "                    # Displaying the bounding box only if the detection confidence is more than 0.2\n",
    "                    if conf > 0.2:\n",
    "                        centerX = int(eachDetection[0]*width)\n",
    "                        centerY = int(eachDetection[1]*height)\n",
    "                        \n",
    "                        # Calculating the 'x', 'y', 'w' and 'h' values for each bounding box\n",
    "                        w = int(eachDetection[2]*width)\n",
    "                        h = int(eachDetection[3]*height)\n",
    "                        x = int(centerX - w/2)\n",
    "                        y = int(centerY - h/2)\n",
    "\n",
    "                        # Appending the dimensions of the bounding box, the class id and the confidence to the respective lists\n",
    "                        boundingBoxes.append([x, y, w, h])\n",
    "                        classIds.append(classId)\n",
    "                        confidence.append((float(conf)))\n",
    "                        \n",
    "            # Performing non-maximum suppression for the bounding boxes and its corresponding confidence levels            \n",
    "            indexes = cv2.dnn.NMSBoxes(boundingBoxes, confidence, 0.2, 0.4)\n",
    "            \n",
    "            # Checking if at least one index exists\n",
    "            if len(indexes)>0:\n",
    "                \n",
    "                # For each index, the colour, the bounding box, the class label and the confidence level is written on the frame\n",
    "                for index in indexes.flatten():\n",
    "                    colour = colours[index]\n",
    "                    x, y, w, h = boundingBoxes[index]\n",
    "                    trafficLight = str(trafficLights[classIds[index]])\n",
    "                    con = str(round(confidence[index],2))\n",
    "                    cv2.rectangle(frame, (x,y), (x+w, y+h), colour, 2)\n",
    "                    cv2.putText(frame, trafficLight + \" \" + con, (x, y+20), font, 2, (255,255,255), 2)\n",
    "                    \n",
    "                    # The frame with the detected traffic lights is written to a video output\n",
    "                    trafficLightsDetectionVideo.write(frame)\n",
    "    \n",
    "    # Release all the resources\n",
    "    trafficLightsDetectionVideo.release()\n",
    "    videoCapture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return;\n",
    "\n",
    "def trafficLightDetectionAndTrack(frame,deepNeuralNetwork,trafficLights,colours):\n",
    "    \n",
    "    # A flag to indicate the aspect change to red\n",
    "    redLightFlag=False\n",
    "    \n",
    "    # Initializing the bounding boxes, confidence and the class ids for traffic light detection\n",
    "    boundingBoxes = []\n",
    "    classIds = []\n",
    "    confidence = []\n",
    "            \n",
    "    # Obtaining the frame's height and width\n",
    "    height, width, _ = frame.shape\n",
    "            \n",
    "    # Obtaining the four dimensional Binary Large OBject (BLOB) from the frame\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0,0,0), swapRB=True, crop=False)\n",
    "            \n",
    "    # Setting the BLOB as the input to the deep neural network (DNN)\n",
    "    deepNeuralNetwork.setInput(blob)\n",
    "            \n",
    "    # Retrieving the names of the unconnected output layers in the deep neural network and passing it to forward propagation\n",
    "    unconnectedOutputLayersNames = deepNeuralNetwork.getUnconnectedOutLayersNames()\n",
    "    layerOutputs = deepNeuralNetwork.forward(unconnectedOutputLayersNames)\n",
    "            \n",
    "            \n",
    "    for eachOutput in layerOutputs:\n",
    "        for eachDetection in eachOutput:\n",
    "            # Obtaining the score, class id and the confidence for each detection\n",
    "            scores = eachDetection[5:]\n",
    "            classId = np.argmax(scores)\n",
    "            conf = scores[classId]\n",
    "                    \n",
    "            # Displaying the bounding box only if the detection confidence is more than 0.2\n",
    "            if conf > 0.25:\n",
    "                centerX = int(eachDetection[0]*width)\n",
    "                centerY = int(eachDetection[1]*height)\n",
    "                        \n",
    "                # Calculating the 'x', 'y', 'w' and 'h' values for each bounding box\n",
    "                w = int(eachDetection[2]*width)\n",
    "                h = int(eachDetection[3]*height)\n",
    "                x = int(centerX - w/2)\n",
    "                y = int(centerY - h/2)\n",
    "\n",
    "                # Appending the dimensions of the bounding box, the class id and the confidence to the respective lists\n",
    "                boundingBoxes.append([x, y, w, h])\n",
    "                classIds.append(classId)\n",
    "                confidence.append((float(conf)))\n",
    "                        \n",
    "    # Performing non-maximum suppression for the bounding boxes and its corresponding confidence levels            \n",
    "    indexes = cv2.dnn.NMSBoxes(boundingBoxes, confidence, 0.2, 0.4)\n",
    "            \n",
    "    # Checking if at least one index exists\n",
    "    if len(indexes)>0:\n",
    "                \n",
    "        # For each index, the colour, the bounding box, the class label and the confidence level is written on the frame\n",
    "        for index in indexes.flatten():\n",
    "            colour = colours[index]\n",
    "            x, y, w, h = boundingBoxes[index]\n",
    "            trafficLight = str(trafficLights[classIds[index]])\n",
    "            con = str(round(confidence[index],2))\n",
    "            \n",
    "            if (trafficLight=='Green light'):\n",
    "                cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "                cv2.putText(frame, trafficLight + \" \" + con, (x, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0,255,0), 2)\n",
    "            \n",
    "            elif (trafficLight=='Red light'):\n",
    "                cv2.rectangle(frame, (x,y), (x+w, y+h), (0,0,255), 2)\n",
    "                cv2.putText(frame, trafficLight + \" \" + con, (x, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0,0,255), 2)\n",
    "                redLightFlag=True\n",
    "            \n",
    "            elif (trafficLight=='Yellow light'):\n",
    "                cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,255), 2)\n",
    "                cv2.putText(frame, trafficLight + \" \" + con, (x, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0,255,255), 2)\n",
    "        \n",
    "    return frame,redLightFlag;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
